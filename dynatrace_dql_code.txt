fetch logs
| filter log.source=="linux.log"

DQl --->data record(text="Vivek,search,savedsearch")
SPL --->| makeresults | eval text="Vivek,search,savedsearch" | fields - _time


DQL --->describe logs
SPL --->index=main source="Linux.log" | fieldsummary


DQL ---> fetch logs, from:-24h ,to:-2h
SPL---> index=main sourcetype="linux_log" earliest=-24h latest=-2h

DQL ---> fetch logs, from:-24h ,to:-2h, samplingRatio:1000


DQL ---> fetch logs| filter log.source == "linux.log" | summarize total_count = count()
SPL ---> index=main source="linux.log" | stats count as total_count


DQL ---> fetch logs| filter log.source == "linux.log" | summarize count() ,by:status

fetch logs| filter log.source == "linux.log" | summarize count() ,by:{status == "NONE"}

DQL ---> data record (value=2), record(value=3), record(value=4)| summarize total_sum = sum(value), total_avg = avg(value)
SPL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats sum(number) as total_number, avg(number) as total_avg


DQL ---> fetch logs | filter log.source=="linux.log" | summarize dc_count=countDistinctExact(status)

DQL ---> fetch logs  | summarize unique_source_count=countDistinctExact(log.source)
SPL ---> index=main | stats dc(source) as unique_source


DQL ---> fetch logs  | summarize total_count = countIf(log.source=="linux.log")
SPL ---> index=main source="Linux.log" | stats count


DQL ---> data record (value=2), record(value=3), record(value=4) | summarize max(value), min(value), median(value)
SPL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats max(number) as total_number, min(number) as total_avg


DQL ---> data record (value=2), record(value=3), record(value=4) | summarize takeFirst(value), takeLast(value)
SQL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats first(number) as first_number, last(number) as last_number


DQL ---> data record (value=2), record(value=3), record(value=4) | summarize collectArray(value), collectArray(value, maxLength:1)
SQL ---> | makeresults count=3 | streamstats count as id | eval value=case(id=1, 2, id=2, 3, id=3, 4) | stats list(value) as collect_array | eval collect_array=mvindex(collect_array, 0, 1)

| makeresults | eval a = "2,3,4" | eval full_array = split(a,",") | eval max_array = mvindex(full_array,0,0)


DQL ---> fetch logs| sort timestamp desc|dedup status

DQL ---> fetch logs| filterOut log.source == "linux.log" | summarize count() ,by:log.source
SPL ---> index=main source!="Linux.log" | stats count by source

DQL ---> fetch logs| search "ERROR"
SPL ---> index=main source="Linux.log" | search *linux*

DQL ---> fetch logs| filter log.source=="linux.log" | fields event.type, log.source, status
SPL ---> index=main source="Linux.log" |table source, host, sourcetype

DQL ---> data record(event = "Lets find a product",status="Missing")| fieldsAdd event_upper = upper(event), lower_case_status=lower(status)
SPL ---> | makeresults | eval event="Lets find a product", status="Missing" | eval event_upper=upper(event), lower_case_status=lower(status) | fields - _time

DQL ---> fetch logs| dedup status | fieldsAdd state = if(status=="ERROR","Problem", else:"Normal")
SPL ---> index=main | dedup status| eval state=if(status=="ERROR", "Problem", "Normal")| table status, state

DQL ---> data record(a="a", b="b", c="c"), record(b="b", c="c"), record(c="c"), record()| fieldsAdd coalesce(a,b,c)


DQL ---> 
Approach1 ---> fetch logs| fields loglevel| dedup loglevel| fieldsAdd New_status = if(loglevel == "ERROR", "Critical", else:if(loglevel == "INFO", "Information", else:if(loglevel == "NONE", "Low", else:if(loglevel == "WARN", "High", else:loglevel))))

Approach2 ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High")|fieldsAdd final_status=coalesce(state,state2,state3,state4) 

DQL ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High") |fieldsAdd final_status=coalesce(state,state2,state3,state4) | fieldsRemove state, state2, state3, state4
SQL ---> | makeresults | eval event="Lets find a product", status="Missing" | eval event_upper=upper(event), lower_case_status=lower(status) | fields - _time


DQL ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High") |fieldsAdd final_status=coalesce(state,state2,state3,state4) | fieldsRename intial_status=status, `final status`=final_status

DQL ---> data record(name="Vivek")| fieldsAdd hashCrc32(name), hashMd5(name),hashSha1(name), hashSha256(name), hashSha512(name)
SPL ---> | makeresults | eval name="Vivek"| eval md5 = md5(name),sha1 = sha1(name),Sha256 = sha256(name),Sha512 = sha512(name)


DQL ---> data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | limit 3
SQL ---> index=_internal | stats count by sourcetype | head 3


DQL ---> data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | sort  a
data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | sort - a

DQL ---> data record(a=30,b="%")| fieldsAdd concat(a," ",b)
SPL ---> |makeresults| eval a=30, b="%" | eval concatenation = a." ".b


DQL ---> fetch logs| filter contains(log.source,"linux")
SPL ---> index=main source=*linux*

DQL ---> data record(content="Dyntrace query language") | fieldsAdd endsWith(content,"Language", caseSensitive:false), endsWith(content,"Language")
SPL ---> | makeresults | eval TEK = "DQL ODI" | eval ends_with_ODI = if(match(TEK, "ODI"), "true", "false") | eval ends_with_ci = if(match(lower(TEK),"odi"),"true","false")


DQL ---> data record(str="DQL is awesome")| fieldsAdd first_char = getCharacter(str,0),last_char = getCharacter(str,-1),
        middle_char= getCharacter(str,7)
SPL ---> | makeresults| eval s1="DQL is Awesome"|eval s2="Dynatrace Query Language"| eval v1=substr(s1, 1, 1), v2=substr(s1, 9, 1)| eval v3=substr(s2, 1, 1), v4=substr(s2, 5, 1)| table s1,s2, v1, v2, v3, v4


DQL ---> data record(str="DQL is awesome")| fieldsAdd like(str,"D%"), like(str,"%D%X")


DQL ---> data record(str="DQL is awesome")| fieldsAdd matchesPhrase(str,"DQL"), matchesPhrase(str,"dql", caseSensitive:true)

DQL ---> data record(str="DQL is awesome")| fieldsAdd matchesPhrase(str,"DQL"), matchesPhrase(str,"dql", caseSensitive:true), matchesValue(str,"DQL*"), matchesValue(str,"dql*", caseSensitive:true)

DQL ---> data record(str="DQL is awesome")| fieldsAdd  replaceString(str,"awesome","simple")
SPL ---> | makeresults| eval s1="DQL is Awesome"| eval replace_s1 = replace(s1,"Awesome","Simple")

DQL ---> fetch logs| filter log.source=="linux.log"|summarize total_linux_count = count()| append [fetch logs | filter contains(log.source,"win") | summarize total_windows_count = count()]| fieldsAdd count = coalesce(total_linux_count,total_windows_count)
SPL ---> index=main source="linux.log" | stats count as total_linux_log | append [ search index=main source="WinEvents.log"| stats count as total_win_count]


DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")| join [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}
SPL ---> | makeresults | eval key=1, value="a"| append [ makeresults | eval key=2, value="b"]
| append [ makeresults | eval key=3, value="c"]| fields -  _time| join key [  makeresults | eval key=2, cost=20| append [ makeresults | eval key=3, cost=30]| append [makeresults | eval key=4, cost=40]| fields key cost]| fields - _time


DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")
| join [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}, kind:leftOuter
SPL ---> | makeresults | eval key=1, value="a"| append [ makeresults | eval key=2, value="b"]| append [ makeresults | eval key=3, value="c"]| fields -  _time| join key type=left [  makeresults | eval key=2, cost=20| append [ makeresultseval key=3, cost=30]| append [makeresults | eval key=4, cost=40]| fields key cost
]| fields - _time

DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")
| join [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}, kind:outer

DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")| joinnested nestedrecord= [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}

DQL ---> timeseries usage = avg(dt.host.cpu.usage), by:dt.entity.host, interval:1h| fieldsAdd entityName(dt.entity.host) | filter contains(dt.entity.host.name,"easy")|fieldsAdd avg_usage = arrayAvg(usage) 

DQL ---> timeseries usage = avg(dt.host.cpu.usage), by:dt.entity.host, interval:1h| fieldsAdd entityName(dt.entity.host) | filter contains(dt.entity.host.name,"easy")|fieldsAdd avg_usage = arrayAvg(usage)| fieldsAdd round_avg_usage = tolong(avg_usage)


DQL ---> data record(a=array(1,2),b="DQL"), record(a=array(3,4,5),b="Dynatrace Query Lanuage")| expand a
SPL ---> | makeresults count=2| streamstats count as ED| eval a=case(ED=1, "1,2", ED=2, "3,4,5,6,")| eval b=case(ED=1, "Splunk",ED=2, "Splunk Query Language")| eval a=split(a,",")| mvexpand limit=5 a| table a b