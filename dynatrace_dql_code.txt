fetch logs
| filter log.source=="linux.log"

DQl --->
data record(text="Vivek,search,savedsearch")

SPL --->
| makeresults 
| eval text="Vivek,search,savedsearch" 
| fields - _time


DQL --->
describe logs

SPL --->

index=main source="Linux.log" | fieldsummary


DQL ---> fetch logs, from:-24h ,to:-2h
SPL---> index=main sourcetype="linux_log" earliest=-24h latest=-2h

DQL ---> fetch logs, from:-24h ,to:-2h, samplingRatio:1000


DQL ---> fetch logs| filter log.source == "linux.log" | summarize total_count = count()
SPL ---> index=main source="linux.log" | stats count as total_count


DQL ---> fetch logs| filter log.source == "linux.log" | summarize count() ,by:status

fetch logs| filter log.source == "linux.log" | summarize count() ,by:{status == "NONE"}

DQL ---> data record (value=2), record(value=3), record(value=4)| summarize total_sum = sum(value), total_avg = avg(value)

SPL ---> | makeresults count=3 
| streamstats count 
| eval number = case(count=1,2, count=2, 3, count=3, 4) 
| stats sum(number) as total_number, avg(number) as total_avg

DQL ---> fetch logs | filter log.source=="linux.log" | summarize dc_count=countDistinctExact(status)

DQL ---> fetch logs  | summarize unique_source_count=countDistinctExact(log.source)
SPL ---> index=main | stats dc(source) as unique_source


DQL ---> fetch logs  | summarize total_count = countIf(log.source=="linux.log")
SPL ---> index=main source="Linux.log" | stats count

DQL ---> data record (value=2), record(value=3), record(value=4) | summarize max(value), min(value), median(value)
SPL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats max(number) as total_number, min(number) as total_avg

DQL ---> data record (value=2), record(value=3), record(value=4) | summarize takeFirst(value), takeLast(value)
SQL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats first(number) as first_number, last(number) as last_number

DQL ---> data record (value=2), record(value=3), record(value=4) | summarize collectArray(value), collectArray(value, maxLength:1)
SQL ---> | makeresults count=3 | streamstats count as id | eval value=case(id=1, 2, id=2, 3, id=3, 4) | stats list(value) as collect_array | eval collect_array=mvindex(collect_array, 0, 1)

| makeresults | eval a = "2,3,4" | eval full_array = split(a,",") | eval max_array = mvindex(full_array,0,0)


DQL ---> fetch logs| sort timestamp desc|dedup status

DQL ---> fetch logs| filterOut log.source == "linux.log" | summarize count() ,by:log.source
SPL ---> index=main source!="Linux.log" | stats count by source

DQL ---> fetch logs| search "ERROR"
SPL ---> index=main source="Linux.log" | search *linux*

DQL ---> fetch logs| filter log.source=="linux.log" | fields event.type, log.source, status
SPL ---> index=main source="Linux.log" |table source, host, sourcetype

DQL ---> data record(event = "Lets find a product",status="Missing")| fieldsAdd event_upper = upper(event), lower_case_status=lower(status)
SPL ---> | makeresults | eval event="Lets find a product", status="Missing" | eval event_upper=upper(event), lower_case_status=lower(status) | fields - _time

DQL ---> fetch logs| dedup status | fieldsAdd state = if(status=="ERROR","Problem", else:"Normal")
SPL ---> index=main | dedup status| eval state=if(status=="ERROR", "Problem", "Normal")| table status, state

DQL ---> data record(a="a", b="b", c="c"), record(b="b", c="c"), record(c="c"), record()| fieldsAdd coalesce(a,b,c)


DQL ---> 
Approach1 ---> fetch logs| fields loglevel| dedup loglevel| fieldsAdd New_status = if(loglevel == "ERROR", "Critical", else:if(loglevel == "INFO", "Information", else:if(loglevel == "NONE", "Low", else:if(loglevel == "WARN", "High", else:loglevel))))

Approach2 ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High")|fieldsAdd final_status=coalesce(state,state2,state3,state4) 

DQL ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High") |fieldsAdd final_status=coalesce(state,state2,state3,state4) | fieldsRemove state, state2, state3, state4
SQL ---> | makeresults | eval event="Lets find a product", status="Missing" | eval event_upper=upper(event), lower_case_status=lower(status) | fields - _time


DQL ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High") |fieldsAdd final_status=coalesce(state,state2,state3,state4) | fieldsRename intial_status=status, `final status`=final_status

DQL ---> data record(name="Vivek")| fieldsAdd hashCrc32(name), hashMd5(name),hashSha1(name), hashSha256(name), hashSha512(name)
SPL ---> | makeresults | eval name="Vivek"| eval md5 = md5(name),sha1 = sha1(name),Sha256 = sha256(name),Sha512 = sha512(name)

DQL ---> data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | limit 3
SQL ---> index=_internal | stats count by sourcetype | head 3

DQL ---> data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | sort  a
data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | sort - a

DQL ---> data record(a=30,b="%")| fieldsAdd concat(a," ",b)
SPL ---> |makeresults| eval a=30, b="%" | eval concatenation = a." ".b

