fetch logs
| filter log.source=="linux.log"

DQl --->data record(text="Vivek,search,savedsearch")
SPL --->| makeresults | eval text="Vivek,search,savedsearch" | fields - _time


DQL --->describe logs
SPL --->index=main source="Linux.log" | fieldsummary


DQL ---> fetch logs, from:-24h ,to:-2h
SPL---> index=main sourcetype="linux_log" earliest=-24h latest=-2h

DQL ---> fetch logs, from:-24h ,to:-2h, samplingRatio:1000


DQL ---> fetch logs| filter log.source == "linux.log" | summarize total_count = count()
SPL ---> index=main source="linux.log" | stats count as total_count


DQL ---> fetch logs| filter log.source == "linux.log" | summarize count() ,by:status

fetch logs| filter log.source == "linux.log" | summarize count() ,by:{status == "NONE"}

DQL ---> data record (value=2), record(value=3), record(value=4)| summarize total_sum = sum(value), total_avg = avg(value)
SPL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats sum(number) as total_number, avg(number) as total_avg


DQL ---> fetch logs | filter log.source=="linux.log" | summarize dc_count=countDistinctExact(status)

DQL ---> fetch logs  | summarize unique_source_count=countDistinctExact(log.source)
SPL ---> index=main | stats dc(source) as unique_source


DQL ---> fetch logs  | summarize total_count = countIf(log.source=="linux.log")
SPL ---> index=main source="Linux.log" | stats count


DQL ---> data record (value=2), record(value=3), record(value=4) | summarize max(value), min(value), median(value)
SPL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats max(number) as total_number, min(number) as total_avg


DQL ---> data record (value=2), record(value=3), record(value=4) | summarize takeFirst(value), takeLast(value)
SQL ---> | makeresults count=3 | streamstats count | eval number = case(count=1,2, count=2, 3, count=3, 4) | stats first(number) as first_number, last(number) as last_number


DQL ---> data record (value=2), record(value=3), record(value=4) | summarize collectArray(value), collectArray(value, maxLength:1)
SQL ---> | makeresults count=3 | streamstats count as id | eval value=case(id=1, 2, id=2, 3, id=3, 4) | stats list(value) as collect_array | eval collect_array=mvindex(collect_array, 0, 1)

| makeresults | eval a = "2,3,4" | eval full_array = split(a,",") | eval max_array = mvindex(full_array,0,0)


DQL ---> fetch logs| sort timestamp desc|dedup status

DQL ---> fetch logs| filterOut log.source == "linux.log" | summarize count() ,by:log.source
SPL ---> index=main source!="Linux.log" | stats count by source

DQL ---> fetch logs| search "ERROR"
SPL ---> index=main source="Linux.log" | search *linux*

DQL ---> fetch logs| filter log.source=="linux.log" | fields event.type, log.source, status
SPL ---> index=main source="Linux.log" |table source, host, sourcetype

DQL ---> data record(event = "Lets find a product",status="Missing")| fieldsAdd event_upper = upper(event), lower_case_status=lower(status)
SPL ---> | makeresults | eval event="Lets find a product", status="Missing" | eval event_upper=upper(event), lower_case_status=lower(status) | fields - _time

DQL ---> fetch logs| dedup status | fieldsAdd state = if(status=="ERROR","Problem", else:"Normal")
SPL ---> index=main | dedup status| eval state=if(status=="ERROR", "Problem", "Normal")| table status, state

DQL ---> data record(a="a", b="b", c="c"), record(b="b", c="c"), record(c="c"), record()| fieldsAdd coalesce(a,b,c)


DQL ---> 
Approach1 ---> fetch logs| fields loglevel| dedup loglevel| fieldsAdd New_status = if(loglevel == "ERROR", "Critical", else:if(loglevel == "INFO", "Information", else:if(loglevel == "NONE", "Low", else:if(loglevel == "WARN", "High", else:loglevel))))

Approach2 ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High")|fieldsAdd final_status=coalesce(state,state2,state3,state4) 

DQL ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High") |fieldsAdd final_status=coalesce(state,state2,state3,state4) | fieldsRemove state, state2, state3, state4
SQL ---> | makeresults | eval event="Lets find a product", status="Missing" | eval event_upper=upper(event), lower_case_status=lower(status) | fields - _time


DQL ---> fetch logs | dedup status | fields status |fieldsAdd state=if(status=="ERROR","Critical"), state2=if(status=="INFO","Information"), state3=if(status=="NONE","Low"), state4=if(status=="WARN","High") |fieldsAdd final_status=coalesce(state,state2,state3,state4) | fieldsRename intial_status=status, `final status`=final_status

DQL ---> data record(name="Vivek")| fieldsAdd hashCrc32(name), hashMd5(name),hashSha1(name), hashSha256(name), hashSha512(name)
SPL ---> | makeresults | eval name="Vivek"| eval md5 = md5(name),sha1 = sha1(name),Sha256 = sha256(name),Sha512 = sha512(name)


DQL ---> data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | limit 3
SQL ---> index=_internal | stats count by sourcetype | head 3


DQL ---> data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | sort  a
data record (a=2), record(a=3), record(a=7), record(a=1), record(a=5) | sort - a

DQL ---> data record(a=30,b="%")| fieldsAdd concat(a," ",b)
SPL ---> |makeresults| eval a=30, b="%" | eval concatenation = a." ".b


DQL ---> fetch logs| filter contains(log.source,"linux")
SPL ---> index=main source=*linux*

DQL ---> data record(content="Dyntrace query language") | fieldsAdd endsWith(content,"Language", caseSensitive:false), endsWith(content,"Language")
SPL ---> | makeresults | eval TEK = "DQL ODI" | eval ends_with_ODI = if(match(TEK, "ODI"), "true", "false") | eval ends_with_ci = if(match(lower(TEK),"odi"),"true","false")


DQL ---> data record(str="DQL is awesome")| fieldsAdd first_char = getCharacter(str,0),last_char = getCharacter(str,-1),
        middle_char= getCharacter(str,7)
SPL ---> | makeresults| eval s1="DQL is Awesome"|eval s2="Dynatrace Query Language"| eval v1=substr(s1, 1, 1), v2=substr(s1, 9, 1)| eval v3=substr(s2, 1, 1), v4=substr(s2, 5, 1)| table s1,s2, v1, v2, v3, v4


DQL ---> data record(str="DQL is awesome")| fieldsAdd like(str,"D%"), like(str,"%D%X")


DQL ---> data record(str="DQL is awesome")| fieldsAdd matchesPhrase(str,"DQL"), matchesPhrase(str,"dql", caseSensitive:true)

DQL ---> data record(str="DQL is awesome")| fieldsAdd matchesPhrase(str,"DQL"), matchesPhrase(str,"dql", caseSensitive:true), matchesValue(str,"DQL*"), matchesValue(str,"dql*", caseSensitive:true)

DQL ---> data record(str="DQL is awesome")| fieldsAdd  replaceString(str,"awesome","simple")
SPL ---> | makeresults| eval s1="DQL is Awesome"| eval replace_s1 = replace(s1,"Awesome","Simple")

DQL ---> fetch logs| filter log.source=="linux.log"|summarize total_linux_count = count()| append [fetch logs | filter contains(log.source,"win") | summarize total_windows_count = count()]| fieldsAdd count = coalesce(total_linux_count,total_windows_count)
SPL ---> index=main source="linux.log" | stats count as total_linux_log | append [ search index=main source="WinEvents.log"| stats count as total_win_count]


DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")| join [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}
SPL ---> | makeresults | eval key=1, value="a"| append [ makeresults | eval key=2, value="b"]
| append [ makeresults | eval key=3, value="c"]| fields -  _time| join key [  makeresults | eval key=2, cost=20| append [ makeresults | eval key=3, cost=30]| append [makeresults | eval key=4, cost=40]| fields key cost]| fields - _time


DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")
| join [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}, kind:leftOuter
SPL ---> | makeresults | eval key=1, value="a"| append [ makeresults | eval key=2, value="b"]| append [ makeresults | eval key=3, value="c"]| fields -  _time| join key type=left [  makeresults | eval key=2, cost=20| append [ makeresultseval key=3, cost=30]| append [makeresults | eval key=4, cost=40]| fields key cost
]| fields - _time

DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")
| join [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}, kind:outer

DQL ---> data record (key=1, value="a"), record(key=2, value="b"), record(key=3, value="c")| joinnested nestedrecord= [data record (key=2, cost=20), record(key=3, cost=30), record(key=4, cost=40)], on:{key}

DQL ---> timeseries usage = avg(dt.host.cpu.usage), by:dt.entity.host, interval:1h| fieldsAdd entityName(dt.entity.host) | filter contains(dt.entity.host.name,"easy")|fieldsAdd avg_usage = arrayAvg(usage) 

DQL ---> timeseries usage = avg(dt.host.cpu.usage), by:dt.entity.host, interval:1h| fieldsAdd entityName(dt.entity.host) | filter contains(dt.entity.host.name,"easy")|fieldsAdd avg_usage = arrayAvg(usage)| fieldsAdd round_avg_usage = tolong(avg_usage)


DQL ---> data record(a=array(1,2),b="DQL"), record(a=array(3,4,5),b="Dynatrace Query Lanuage")| expand a
SPL ---> | makeresults count=2| streamstats count as ED| eval a=case(ED=1, "1,2", ED=2, "3,4,5,6,")| eval b=case(ED=1, "Splunk",ED=2, "Splunk Query Language")| eval a=split(a,",")| mvexpand limit=5 a| table a b

DQL ---> fetch logs| filter matchesValue(log.source,"apache.log") AND contains(content,"/favicon.ico")| summarize total_count=count()| filter total_count>500
SPL ---> index="main" sourcetype="access_combined"| rex field=_raw "\"\w+\s(?<url>[^\s]+)\sHTTP" | stats count by url | where url ="/favicon.ico" | where count > 500

DQL ---> data record(r=record(a="DQL", b=1,c=record(d=0))),record(r=record(a="Dynatrace Query Language",b=2,c="1"))| fieldsFlatten r,depth:2

DQL ---> data record()| fieldsAdd a=array(2,3,7,7,1), field_a=array("Hello World")|fieldsAdd b=arrayAvg(a)| fieldsAdd c=arrayMax(a),d=arrayMin(a), e=arrayDistinct(a),arrayConcat(a,field_a)

DQL ---> data record(a=1), record(a=2), record(a=3), record(a=4)| summarize a_array =collectArray(a)

DQL ---> data record(value="DQL"),record(),record(value="null") | fieldsAdd isNull(value), isNotNull(value), isTrueOrNull(value)

DQL ---> fetch logs | fieldsAdd entityName(dt.entity.host), entityName(dt.entity.aws_availability_zone)

DQL ---> data record(a="java"), record(a="python"), record(a="DQL"), record(a="SPLUNK")| fieldsAdd in(a,{"java","go","splunk"})

DQL ---> fetch logs | limit 1| fields exists(content), exists(timestamp), exists(severity)
SPL ---> index=main | head 1 | where isnotnull(_raw) AND isnotnull(_time) AND isnotnull(severity)

DQL ---> data record (x=-42.13), record(x=0), record(x=6.234567)| fieldsAdd abs(x)

DQL ---> data record (x=-0.5), record(x=0), record(x=0.5)| fieldsAdd ceil(x), floor(x)

DQL ---> fetch logs| limit 10| fieldsAdd randomValue=ceil(random()*100+1)

DQL ---> data record(value="127.0.0.1"), record(value="10.0.0.5"),record(value="2001:0db8:0000:0000:0000:8a2e:0370:7334"), record(value="::1"), record(value = "317.0.0.1")| fieldsAdd ip(value)

DQL ---> data record(value="127.1.2.3"), record(value="10.4.0.5"),record(value="2001:0db8:0000:0000:0000:8a2e:0370:7334"), record(value="::1"), record(value = "317.0.0.1")| fieldsAdd ip(value), ipMask(value,8), ipMask(value,16,ipv6MaskBits:32)

DQL ---> data record(value="127.1.2.3"), record(value="10.4.0.5"), record(value="2001:0db8:0000:0000:0000:8a2e:0370:7334"), record(value="::1"), record(value = "317.0.0.1"), record(value="127.0.0.1")| fieldsAdd ip(value), ipMask(value,8), ipMask(value,16,ipv6MaskBits:32), ipIsPrivate(value), ipIsPublic(value), isIp(value), isIpV4(value), isIpV6(value)


DQL ---> data record(value="127.1.2.3"), record(value="10.4.0.5"), record(value="2001:0db8:0000:0000:0000:8a2e:0370:7334"), record(value="::1"), record(value = "317.0.0.1"), record(value="127.0.0.1"), record(value="255.255.255.255")| fieldsAdd ip(value), ipMask(value,8), ipMask(value,16,ipv6MaskBits:32), ipIsPrivate(value), ipIsPublic(value), isIp(value), isIpV4(value), isIpV6(value)

DQL ---> data record(value=1000, unit="ns"), record (value=60, unit="s"), record(value=24, unit="h") | fieldsAdd duration(value,unit)

DQL ---> data record(timestamp=toTimestamp("2025-08-08T10:59:30.000-0400"))
| fieldsAdd formatted = formatTimestamp(timestamp, format:"MM-dd-yyyy"), year = formatTimestamp(timestamp,format:"y"), month = formatTimestamp(timestamp, format:"M"), week = formatTimestamp(timestamp, format:"w"),
            dayofWeek = formatTimestamp(timestamp, format:"E", locale:"en-US"), hour=formatTimestamp(timestamp, format:"H"), time=formatTimestamp(timestamp, format:"HH:mm, VV"), 
            timeET= formatTimestamp(timestamp, format:"HH:MM, VV", timezone:"US/Eastern")
			
			
			
DQL ---> data record(timestamp=toTimestamp("2025-08-08T10:59:30.000-0400"))
| fieldsAdd formatted = formatTimestamp(timestamp, format:"MM-dd-yyyy"), year = formatTimestamp(timestamp,format:"y"), month = formatTimestamp(timestamp, format:"M"), week = formatTimestamp(timestamp, format:"w"),
            dayofWeek = formatTimestamp(timestamp, format:"E", locale:"en-US"), hour=formatTimestamp(timestamp, format:"H"), time=formatTimestamp(timestamp, format:"HH:mm, VV"), 
            timeET= formatTimestamp(timestamp, format:"HH:MM, VV", timezone:"US/Eastern"), getHour(timestamp), getMinute(timestamp), getDayOfMonth(timestamp), getYear(timestamp), getSecond(timestamp), 
            getDayOfWeek(timestamp), getDayOfYear(timestamp)


			
DQL ---> data record(timeframe = timeframe(from:"2019-08-01T09:30:00", to:"2019-08-03T09:38:00"))| fieldsAdd getStart(timeframe), getEnd(timeframe), now()


DQL ---> data record(location="Bangalore", state="KA"),
      record(location="Mumbai", state="MH"),
      record(location="Paris", state="PR")

| fieldsAdd result=lookup([

data record(city="Bangalore", country="India"),
     record(city="Paris", country="France")
],

sourceField:location,
lookupField:city
)
      
	  
	  